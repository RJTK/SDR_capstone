\documentclass[a4paper, 12pt]{article}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{shapes.geometric}
\usepackage{fullpage}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\author{Ryan Kinnear - 200273748 \\ Raza Rauf}
\title{Implementation of Software Defined Radio}
\date{\today}

\newtheorem{thm:NSST}{Nyquist-Shannon Sampling Theorem}

\begin{document}

\maketitle

\newpage
\tableofcontents
\newpage
\listoffigures
\newpage

\section{Introduction}
\label{sec:intro}
Modern wireless systems are a ubiquitous part of society.  There billions of people accessing the internet, many of them wirelessly.  This includes through home, office, or public WiFi networks, as well as through cellular networks.  The electromagnetic spectrum is also exploited for radio astronomy, Radio Frequency Identification (RFID), radar etc...  

In the past, all radio \footnote{The word \textit{radio} is used to refer to any device that makes use of the EM spectrum.  This does not refer only to communication systems, and certainly not only to radios made for receiving music} functionality was necessarily implemented in hardware using traditional electronics (transistors, capacitors, transformers...).  This also includes implementation with integrated circuits, even highly integrated devices with millions of transistors.  While the highly integrated ICs seem very modern, these radios offer no flexibility.  It is very costly to upgrade this type of radio equipment to take advantage of more modern protocols and algorithms - the entire device needs to be replaced.

The modern proliferation of high speed digital electronics, VLSI, and the mathematical understanding of digital signal processing has naturally lead to the integration of digital processing power and radio devices.  A radio that has some dynamic reconfigurability is referred to as a \textit{software controlled radio}.  This may be a radio capable of dynamically choosing between the size of a QAM constellation based on noise levels, tuning the weights of a digital equalizing filter, or dynamically varying transmit power levels.  However, the fundamental demodulator is fixed in hardware and cannot be modified.  So the problem of upgrading radio equipment to make use of newer protocols and algorithms still exists.

Software defined radio (SDR) is a solution to the problem of limited flexibility.  In SDR, the vast majority of the signal processing is implemented in reconfigurable hardware (FPGAs \cite{fpga_defn}) or in software running on either a Digital Signal Processor (DSP \cite{dsp_defn}) or a General Purpose Processor (GPP) such as the one in your computer.  These devices are very easy to reconfigure, and thus the cost of upgrading to newer protocols and algorithms is drastically reduced.  Not only that but these upgrades or bug fixes can be performed remotely with ease.  Moreover, the ease with which the system is reconfigured allows one to use SDR as a test bed for development.

A further advantage of SDR is that both analog systems and integrated circuits are \textit{extremely} hard to design.  For a mixed signal integrated circuit that is 90\% digital, and 10\% analog, the analog design will take 90\% of the design time \cite{ana_why}.  To design a new integrated circuit every time a new protocol needs to be implemented would be \textit{ridiculous}.

%Write about how the report proceeds.
%Overview of SDR
%Our implementation
%Design goals and strategies
%Testing
%Uses of SDR
%Cognitive Radio

\section{Project Overview}


\section{Fundamentals of SDR}
\label{sec:sdr_funadamentals}
\subsection{The Ideal SDR}
\label{sec:ideal_sdr}
An ideal SDR consists almost entirely of reconfigurable hardware, and digital signal processing. The ultimate goal is to acheive flexibility, and minimize the amount of analog design.  This ideal SDR would consist of an antenna, an analog to digital and digital to analog converter, a circulator, and a processor (see figure \ref{fig:ideal_sdr}).

%SDR architecture image
%------------------------------------------------------------------
%Damn this was hard to draw.

\tikzstyle{ADC} = [draw, fill=blue!20, text width=5em, 
    text centered, minimum height=2.5em]
\tikzstyle{DAC} = [draw, draw, fill=blue!20, text width=5em, 
    text centered, minimum height=2.5em]
\tikzstyle{Data} = [text width=5em, text centered, minimum height = 2.5em]
\tikzstyle{DSP} = [draw, text width=6em, fill=red!20, 
    minimum height=12em, rounded corners, text centered]
\tikzstyle{out} = [coordinate]
\tikzstyle{circulator} = [circle, draw, minimum height = 3em]
\tikzstyle{antenna} = [regular polygon, regular polygon sides = 3, thick, draw, fill=green!20, text width = 2em, shape border rotate = 180]

%\tikzstyle{antenna} = [draw]

\def\blockdist{2.3}
\def\edgedist{2.5}

\begin{figure}[ht]
\caption{Ideal SDR Architecture}
\label{fig:ideal_sdr}
\centering
\begin{tikzpicture}[auto, thick, node distance=2cm, >=latex']
  %The processor node goes right in the middle
  \node (processor) [DSP] {DSP / GPP};
  %Then place the ADC/DAC relative to the processor
  \path (processor.140)+(-\blockdist, 0) node (ADC) [ADC] {ADC};
  \path (processor.-140)+(-\blockdist, 0) node (DAC) [DAC] {DAC};
  %And draw arrows between them
  \draw [->] (ADC) -- (processor.west |- ADC);
  \draw [<-] (DAC) -- (processor.west |- DAC);
  
  %Place the circulator
  \path (processor)+(-3*\blockdist,0) node (circulator) [circulator] {CIRC};
  \draw [<-] (ADC.west) -- (circulator);
  \draw [->] (DAC.west) -- (circulator);

  \path (circulator)+(-1.2*\blockdist,0) node (antenna) [antenna] {ANT};
  \draw [<->] (antenna) -- (circulator.west);

  %Then place outputs relative to the processor
  \path (processor.150)+(2*\blockdist,0) node (out) [Data] {Data Out};
  \path (processor.-150)+(2*\blockdist,0) node (in) [Data] {Data In};
  %And arrows between them
  \draw [<-] (out) -- (processor.east |- out);
  \draw [->] (in) -- (processor.east |- in);

\end{tikzpicture}
\end{figure}
%------------------------------------------------------------------

This has the absolute minimum amount of analog components, and all of the signal processing is done in software.  While there is nothing wrong with this architecture in theory, it is obviously very difficult to implement in practice.  

Historically, the main problem was with the speed of the data converters.  However, there are now ADCs that can directly sample RF frequency signals \cite{gsps_adc}.  These ADCs are very sophisticated, and are far beyond the scope of this project, but high performance commercial software defined radios do make use of ultra high speed data converters.  For the rest of us, there is \textit{undersampling} (section \ref{sec:undersampling}).  This enables one to sample RF signals at rates far lower than twice the highest frequency component.

On the transmission side, it isn't possible to connect a DAC directly to an antenna.  There needs to be at least some analog components to provide power and filtering.

Another issue is the data rate.  A sample rate of 5GSPs might be able to effectively digitize an RF signal, but the torrent of data (40+ Gbps) is far beyond the capability of any GPP or DSP.  This is where FPGAs and \textit{downsampling} (section \ref{sec:downsampling}) come in.

\subsection{Undersampling}
\label{sec:undersampling}
The Nyquist-Shannon Sampling Theorem is the most important theorem in digital signal processing.

\begin{thm:NSST}
\label{thm:NSST}
  If a function $f(t)$ is bandlimited to B hertz then $f(t)$ can be completely reconstructed from discrete samples of $f(t)$ at intervals $\frac{1}{2B}$.
\end{thm:NSST}

If we let $T = \frac{1}{2B}$ be the sampling interval, The specific formula for reconstruction is given by the sinc interpolator:

\begin{equation}
\label{eq:sinc_interpolation}
f(t) = \Big[\sum_{n=-\infty}^{\infty}{f(nT)\delta(t - nT)\Big]}*sinc(\frac{\pi t}{T}) = \sum_{n=-\infty}^{\infty}{f(nT)sinc(\pi\frac{t - nT}{T})}
\end{equation}

A critical piece of this theorem that seems to be frequently misunderstood is that the signal must be \textit{band limited} to B hertz.  This does \textit{not} mean that the highest frequency component needs to be below B hertz.  If the highest frequency component of the signal is below B hertz, then the assumptions of the theorem still hold.  It is a sufficient condition, however it is \textit{not necessary}.

If the signal is band limited to B hertz, but has spectral components greater than B (that is, is it a band pass signal, see figure \ref{fig:bandpass_signal}), it is still possible to sample the signal without losing any information.  However, the effects of \textit{aliasing} will still be present.  But, aliasing is really not a big problem, as will be seen shortly.

\begin{figure}[ht]
\caption{A band pass signal with bandwidth B}
\label{fig:bandpass_signal}
\centering
\includegraphics[width=11cm]{images/bandpass_signal.eps}
\end{figure}

The effect of sampling is to break up the frequency spectrum into ranges of width B.  That is, the frequency axis is partitioned into sets of the form $[nB, (n+1)B], n \in \mathbb{Z}$.  Each of these sets is referred to as a \textit{Nyquist Zone}.  They are labeled $N_1, N_2, ...$ in figure \ref{fig:bandpass_signal}.  Note that $N_1$ is the baseband.  Each of these zones will \textit{alias} or \textit{fold} back to baseband ($N_1$).  The best way to visualize this is to imagine folding the right half plane vertically at each $B, 2B, 3B ...$.  See figure \ref{fig:bandpass_sampling}.  Notice that the even nyquist zones get mirrored about the $B/2$ axis.

\begin{figure}[ht]
\caption{The effect of sampling}
\label{fig:bandpass_sampling}
\centering
\includegraphics[width=13cm]{images/bandpass_sampling.eps}
\end{figure}

It should now be clear that analog to digital converters can be used to effectively digitize signals, even if they are at a very high frequency.  The only requirement is on the \textit{bandwidth} of the signal.  Naturally, a band pass filter is needed to cut out the out of band signals.  In undersampling a band pass filter is used for anti-aliasing. It seems to be commonly assumed that lowpass filters are always used for anti-aliasing, but this is only the case when sampling a signal that falls entirely in $N_1$.

There is another question to be looked at here, which is how to actually choose the sample rate?  If the spectrum of the signal you wish to digitize doesn't conveniently fall into one of the Nyquist zones, then some of the spectrum will be mirrored, and the other part won't be.  This will lead to a loss of information.  Also, it is convenient to ensure that the signal falls into an odd Nyquist zone, so that it doesn't get mirrored.  The criteria for the sampling rate can then be easily written down mathematically, where $f_c$ is the center frequency.

\begin{equation}
\label{eq:sample_rate}
\begin{aligned}
  &f_s > 2B \\
  &f_c - \frac{1}{2}B > nf_s \\
  &f_c + \frac{1}{2}B < \frac{1}{2}(2n + 1)f_s \\
\end{aligned}
\end{equation}

If for a given sample rate $f_s$, $\exists n \in \mathbb{N}$ satisfying the equations in (\ref{eq:sample_rate}) then that sample rate can be used to band pass sample the signal.  Moreover, $n+1$ gives the nyquist zone the signal falls into.

In practice, it is important to keep $n$ low.  There are practical problems associated with undersampling.  When the signal falls into a higher Nyquist zone the jitter on the ADC clock has a larger affect on the quality of the conversion.  The bandwidth of the input and any external components must also be high enough to pass the high frequency signal without adding too much distortion.

The ultimate reason that undersampling is often used in software defined radio is that it takes burden off of the analog to digital converter.

\subsection{A Note on Oversampling}
%Oversampling doesn't actually help that much.
Why oversample when undersampling can do the job\cite{why_oversample}?  This is a good question.  There are some advantages of oversampling.  For example, if a signal is sampled $4n$ times it's bandwidth, and the samples are averaged \footnote{4 samples are averaged together to produce a single sample.  An example of downsampling, section \ref{sec:downsampling}}, the effective resolution of the conversion will be increased by $n$ bits \cite{oversample_extra_bits}.  So, to gain 1 extra bit of resolution (which would provide a theoretical maximum increase in SNR by about 6dB) the bit rate must increase by 400\%, 4 floating point operations need to be performed to produce 1 data sample (3 adds and a divide), the power consumption increases, and digital signal integrity issues get more complex.

The other main advantage of oversampling is that it reduces the stress on the anti-aliasing filter.  However in RF systems, a Surface Accoustic Wave (SAW) filter is used for filtering, and these filters have extremely sharp cutoffs.

When sampling lower bandwidth signals, or when high quality filters are not available, the tradeoffs may be worth it.  For low bandwidth signals even if the sample rate is 20x higher than necessary, it still isnt ``high''.  But in the RF frequency domain, where sample rates already need to be quite high, the trade offs are usually not worth it.

\subsection{Downsampling and Multirate Signal Processing}
\label{sec:downsampling}
Changing the sample rate in a digital signal processing chain is a fundamental operation.  For example, CDs are recorded at 44.1KSPs but recording studios use much higher sample rates.  How do they change the sample rate from the studio recording to the sample rate required for CDs?  The answer is multirate digital signal processing \cite{multirate1} \cite{multirate2}.

To downsample a signal by a factor $M$, you simply throw away every $M^{th}$ sample (equation \ref{eq:downsampling}).  And, to upsample a signal by a factor $L$, you insert $L - 1$ 0's between every sample (equation \ref{eq:upsampling}).

\begin{equation}
\label{eq:downsampling}
y_D[n] = x[Mn] \qquad \text{Downsampling}
\end{equation}

\begin{equation}
\label{eq:upsampling}
y_U[n] = 
  \begin{cases}
    x[k] & \text{if } n = kL, k \in \mathbb{Z} \\
    0 & \text{otherwise}
  \end{cases}
  \qquad \text{Upsampling}
\end{equation}

By applying the properties of the Z-transform \cite{multirate2}, it can be shown that downsampling has the effect of ``compressing'' the frequency domain (equation \ref{eq:downsample_freq}), and upsampling has the effect of ``expanding'' it (equation \ref{eq:upsample_freq}).  Also remember that in the digital domain, there is an infinite number of copies of the spectrum placed at multiples of the sample rate.  These copies are still present in $X(\omega)$ and one needs to be very careful when visualizing equations \ref{eq:downsample_freq} \& \ref{eq:upsample_freq}.

\begin{equation}
\label{eq:downsample_freq}
Y_D(e^{j\omega}) = \frac{1}{M}\sum_{k=0}^{M-1}X(e^{j(\omega - 2\pi k)/M}) \qquad \text{Downsampling}
\end{equation}

\begin{equation}
\label{eq:upsample_freq}
Y_U(e^{j\omega}) = X(e^{j\omega L}) \qquad \text{Upsampling}
\end{equation}

Figure \ref{fig:multirate_freq} shows these effects visually in the frequency domaina.

\begin{figure}[h]
\caption{Multirate in the Frequency Domain}
\label{fig:multirate_freq}
\centering
\includegraphics[width = 14cm]{images/multirate_freq.eps}
\end{figure}

Notice that downsampling causes aliasing.  In order to downsample, the signal first needs to be lowpass filtered to have a bandwidth less than $\frac{\pi}{M}$.  Likewise, when upsampling there are extra unwanted copies of the original signal that need to be filtered out.  For this reason, downsampling is usually preceeded by a lowpass filter, and upsampling is proceeded by a lowpass filter.

So, what is the point of downsampling?  The reason is that processing high speed digital data requires very fast hardware.  If the ADC is running at a modest 30MSPs with 12bits of width, then this is 360Mb/s.  And, if the entire signal processing chain requires a mere 20 operations per sample, then a processor capable of 600 million operations per second is required.  Considering that most embedded processors have a maximum clock speed well below 600MHz, this is a fairly tall order.

Downsampling a signal can significantly reduce the data rate without losing any information.  This is the reason that FPGAs are so commonly used in software defined radio.  FPGAs are capable of massive parallelism and extremely high speeds.  FPGAs are ideal for implementing filters and up/down samplers (resampling) \footnote{resampling is the processes of either downsampling, upsampling, or a combination thereof.  Fractional resampling is acheived by cascading downsamplers and upsamplers.}.

FPGAs can be used to implement portions of the signal processing chain that require the fastest processing.  For example, FFTs, filters, channel selection, resampling, quadrature sampling (section \ref{sec:quadrature_sampling}), etc...  The downsampled signal can then be sent on to the main processor.

%A software defined radio (SDR) is essentially a digital radio.  An analog front end device such as a DVB-T USB dongle \cite{usb_dongle}, HackRF \cite{hackrf}, or the USRP \cite{usrp} receives and digitizes a radio frequency signal and sends the samples to a computer.  The ideal software defined radio consists of an analog to digital convert with one end connected directly to an antenna, and the other end connected to a computer.  A realistic implementation requires a significant analog component to digitize RF signals, as well as significant preprocessing on the digital signal (usually accomplished by an FPGA or DSP).

\subsection{Quadrature Demodulation}
\label{sec:quadrature_sampling}
A complex (as in complex numbers) representation of digital data is the last fundamentally important concept for SDR, and DSP in general.  A general sinusoid can be represented by the following formula \ref{eq:quadrature_sampling}.

\begin{equation}
\label{eq:quadrature_sampling}
A(t)cos(\omega t + \phi (t))
\end{equation}

Now, how can the phase $\phi(t)$ be discerned though regular samples of this signal?  And, how do you determine if $\omega$ is positive or negative?  Most importantly, how can these things be represented by discrete samples?  The answer is I/Q sampling.  The I stands for ``In phase'' and the Q stands for ``Quadrature''.  Formula \ref{eq:quadrature_sampling} can also be written as follows, by using some simple trigonometric identities.

\begin{equation}
\label{eq:trig_ident}
\begin{aligned}
  cos(\alpha)&cos(\beta) = \frac{1}{2}[cos(\alpha - \beta) + cos(\alpha + \beta)]\\
  sin(\alpha)&sin(\beta) = \frac{1}{2}[cos(\alpha - \beta) - cos(\alpha + \beta)]\\
\end{aligned}
\qquad \text{Product to Sum}
\end{equation}

It is then clear that

\begin{equation*} %* disables numbering
A(t)cos(\omega t + \phi (t)) = A(t)cos(\omega t)cos(\phi (t)) - 
A(t)sin(\omega t)sin(\phi(t))
\end{equation*}

Then we define

\begin{equation}
\label{eq:iq_defn}
\begin{aligned}
  &I(t) = A(t)cos(\phi (t))\\
  &Q(t) = -A(t)sin(\phi (t))\\
\end{aligned}
\end{equation}

to arrive at  

\begin{equation}
\label{eq:quadrature_sampling3}
A(t)cos(\omega t + \phi (t)) = I(t)cos(\omega t) + Q(t)sin(\omega t)
\end{equation}

To be clear, this gives us a way to represent the general sinusoid in equation \ref{eq:quadrature_sampling} by using a sum of a sine and a cosine.  To recover the original representation, simply apply the definitions of $I$ and $Q$.

\begin{equation}
\label{eq:quadrature_sampling4}
\begin{aligned}
A(t) &= \sqrt{I^2(t) + Q^2(t})\\
\phi(t) &= \tan^{-1}{\frac{Q(t)}{I(t)}}
\end{aligned}
\end{equation}

Finally, formula \ref{eq:quadrature_sampling4} can be interpreted geometrically (figure \ref{fig:quadrature_sampling}), and it becomes very convenient to use complex numbers to represent I/Q components.  That is, write the separate IQ components as a single complex number $I(t) + jQ(t)$.  This one complex number is a single IQ sample.

\begin{figure}[h]
\caption{Geometric interpretation of formula \ref{eq:quadrature_sampling4}, and a complex number representation}
\label{fig:quadrature_sampling}
\centering
\includegraphics[width=8cm]{images/IQ_sample.eps}
\end{figure}

Since all of these quantities ($A, I, Q, \phi$) are time varying, they can be visualized as a helix in 3 dimensions.  This representation is far more enlightening than the typical 2D plots most of us are used to seeing.  For example, AM modulation can be seen with an IQ representation in figure \ref{fig:am_iq}, and PSK in figure \ref{fig:psk_iq}.

\begin{figure}[h]
\caption{AM modulation visualized in IQ space}
\label{fig:am_iq}
\centering
\includegraphics[width=9cm]{images/AM_IQ.png}
\end{figure}

\begin{figure}[h]
\caption{PSK modulation visualized in IQ space}
\label{fig:psk_iq}
\centering
\includegraphics[width=9cm]{images/PSK_IQ.png}
\end{figure}

The IQ representation is very powerful for digital processing because phase information is easily extracted.  The question remains, how to actually obtain the IQ samples?  The answer is by multiplying the signal separately by a cosine and a sine, both at the carrier frequency, then applying a low pass filter.

\begin{equation*}
\label{eq:quadrature_sampling4}
\begin{aligned}
  &A(t)cos(\omega t + \phi(t))cos(\omega t) = \frac{1}{2}A(t)cos(\phi(t)) + \frac{1}{2}A(t)cos(2\omega t + \phi (t))\\
  &A(t)cos(\omega t + \phi(t))sin(\omega t) = \frac{1}{2}A(t)sin(2\omega t + \phi(t)) - \frac{1}{2}A(t)sin(\phi(t))
\end{aligned}
\end{equation*}

Applying a lowpass filter (with a gain of 2) to these two quantities yields what should be recognized as the IQ components from equation \ref{eq:iq_defn}.

For a more general exposition of the effects of IQ demodulation in the frequency domain, see \cite{iq_sampling}.  However, our project is primarily related to demodulating FM broadcast radio and the preceeding information is enough.

\clearpage
\bibliography{../refs/fourth_year.bib}
\bibliographystyle{IEEEtran}

\end{document}
